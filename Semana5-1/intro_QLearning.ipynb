{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cadenas y procesos de decision de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning es un algoritmo de aprendizaje por refuerzo utilizado para encontrar la política óptima de selección de acciones para un proceso de decisión de Markov finito dado. En un mundo cuadriculado, un agente navega por una cuadrícula para alcanzar un objetivo, evitar obstáculos o maximizar algún tipo de recompensa.\n",
    "\n",
    "Q-Learning fue introducido por Chris Watkins en 1989. Watkins y Peter Dayan presentaron una prueba de convergencia en 1992."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Board](./Images/board.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los componentes clave de un algoritmo de este tipo podrían incluir:\n",
    "\n",
    "* Configuración del entorno: Definición del mundo cuadriculado, incluyendo su tamaño, posiciones inicial y final, obstáculos y posibles estados y acciones para el agente.\n",
    "\n",
    "* Inicialización de la tabla Q: Una tabla que almacena los valores Q para cada par estado-acción. Esta tabla se actualiza a medida que el agente aprende del entorno.\n",
    "\n",
    "* Parámetros de aprendizaje: Incluyen la tasa de aprendizaje (alfa), el factor de descuento (gamma) y detalles de la política, como la tasa de exploración (épsilon) para la política epsilon-greedy.\n",
    "\n",
    "* Bucle de aprendizaje: El bucle principal en el que el agente interactúa con el entorno, actualizando los valores Q en función de las recompensas recibidas y el siguiente estado al que se mueve. Este bucle suele incluir mecanismos para que el agente explore el entorno (tomando acciones aleatorias) y explote su conocimiento actual (tomando la acción más conocida).\n",
    "\n",
    "* Extracción de políticas: Tras el entrenamiento, extracción de la política óptima de la tabla Q, que dicta la mejor acción a tomar a partir de cada estado.\n",
    "\n",
    "* Visualización o prueba: Código para probar la política aprendida y posiblemente visualizar al agente navegando por el mundo cuadriculado, mostrando cómo ha aprendido a alcanzar el objetivo o maximizar su recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning y Ecuaciones de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RL](./Images/rl.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Componentes principales del Q-learning:**\n",
    "\n",
    "* **Agente:** Entidad que aprende a tomar decisiones basadas en el estado del entorno.\n",
    "* **Entorno:** El mundo con el que interactúa el agente, que incluye estados y recompensas.\n",
    "* **Estados:** Representaciones del entorno en un momento dado.\n",
    "* **Acciones:** Las diferentes operaciones que el agente puede realizar en el entorno.\n",
    "* **Recompensas:** Valor que el agente recibe después de tomar una acción en un estado específico, indicando el éxito o fracaso de esa acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcionamiento del Q-learning:\n",
    "\n",
    "1) **Inicialización:** Se crea una tabla Q inicializada con ceros (o valores aleatorios pequeños), donde las filas representan los estados y las columnas las acciones. Los valores Q representan la calidad de una acción específica tomada en un estado específico.\n",
    "\n",
    "2) **Interacción con el entorno:** En cada episodio de entrenamiento, el agente toma acciones en el entorno, observa las recompensas recibidas y actualiza los valores Q en la tabla Q basándose en esas recompensas y las estimaciones futuras de recompensas.\n",
    "\n",
    "3) **Política de acción:** El agente utiliza una política (como ε-greedy) para explorar el entorno tomando acciones aleatorias con una probabilidad ε o explotar el conocimiento actual eligiendo la acción con el mayor valor Q para el estado actual con una probabilidad de 1-ε. Esto equilibra la exploración de nuevas acciones con la explotación de acciones conocidas que maximizan las recompensas.\n",
    "\n",
    "4) **Actualización de la tabla Q:** Después de cada acción, la tabla Q se actualiza utilizando la ecuación de actualización de Q-learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a)=Q(s,a)+α[r+γ max_{a^′}​Q(s^′,a^′)−Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde:\n",
    "\n",
    "* $Q(s,a)$: El valor Q actual para el estado $s$ y la acción $a$.\n",
    "* $α$: Tasa de aprendizaje.\n",
    "* $r$: Recompensa recibida por tomar la acción aa en el estado $s$.\n",
    "* $γ$: Factor de descuento para las recompensas futuras.\n",
    "* $max⁡_{a′}Q(s^′,a^′)$: La mejor estimación de valor Q para el próximo estado $s^′$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q-equation](./Images/Qequation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convergencia:**\n",
    "\n",
    "Con suficiente interacción y actualización, la tabla Q converge hacia los valores óptimos, permitiendo que el agente tome las mejores decisiones para maximizar las recompensas totales a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Q-learning es un algoritmo poderoso y flexible para el aprendizaje por refuerzo que no requiere un modelo del entorno, lo que lo hace adecuado para una amplia gama de aplicaciones, desde juegos hasta problemas de decisión en entornos complejos y dinámicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (1, 3)\n",
    "START = (2, 0)\n",
    "DETERMINISTIC = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define una clase State para un entorno de mundo cuadriculado en un contexto de aprendizaje por refuerzo, específicamente adaptado para un agente que navega por un tablero 2D. La clase contiene métodos para inicializar el estado, manejar las recompensas, comprobar las condiciones de terminación, determinar las siguientes acciones (con un giro probabilístico para el movimiento), calcular la siguiente posición basada en una acción, y visualizar el tablero. Desglosemos cada parte de la clase:\n",
    "\n",
    "**__init__(self, state=START):**\n",
    "* Inicializa un nuevo estado del tablero con todas las posiciones a cero, excepto una posición específica marcada con -1 (posiblemente un obstáculo o una posición de penalización).\n",
    "* El parámetro state se inicializa con START, indicando probablemente la posición inicial del agente.\n",
    "* self.isEnd indica si el estado actual es terminal (es decir, un estado de victoria o derrota).\n",
    "* Las dimensiones del tablero (BOARD_ROWS, BOARD_COLS) y el valor DETERMINISTIC deben definirse en otra parte del código.\n",
    "* La bandera self.determine decide si las acciones dan lugar a resultados deterministas o probabilísticos.\n",
    "\n",
    "**giveReward(self):**\n",
    "\n",
    "* Devuelve una recompensa basada en el estado actual: 1 para un estado ganador, -1 para un estado perdedor, y 0 para todos los demás estados.\n",
    "\n",
    "**isEndFunc(self):**\n",
    "\n",
    "* Comprueba si el estado actual es un estado terminal (victoria o derrota) y actualiza self.isEnd en consecuencia.\n",
    "\n",
    "**_chooseActionProb(self, action):**\n",
    "\n",
    "* Implementa un enfoque probabilístico para decidir la acción real tomada, dada una acción prevista. Esto modela la incertidumbre en el movimiento donde, por ejemplo, intentar moverse \"hacia arriba\" puede resultar en moverse \"hacia la izquierda\" o \"hacia la derecha\" con una pequeña probabilidad.\n",
    "\n",
    "**nxtPosition(self, acción):**\n",
    "\n",
    "* Calcula el siguiente estado basándose en la acción actual. Si self.determine es True, calcula directamente el siguiente estado en función de la acción. Si es False, utiliza _chooseActionProb para determinar probabilísticamente la siguiente acción, haciendo que el entorno no sea determinista.\n",
    "* Comprueba si el siguiente estado está dentro de los límites del tablero y no es un obstáculo; en caso contrario, permanece en el estado actual.\n",
    "\n",
    "**showBoard(self):**\n",
    "\n",
    "* Visualiza el estado actual del tablero con la posición del agente marcada con *, los obstáculos o penalizaciones con z, y las posiciones vacías con 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "\n",
    "    def _chooseActionProb(self, action):\n",
    "        if action == \"up\":\n",
    "            return np.random.choice([\"up\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"down\":\n",
    "            return np.random.choice([\"down\", \"left\", \"right\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"left\":\n",
    "            return np.random.choice([\"left\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n",
    "        if action == \"right\":\n",
    "            return np.random.choice([\"right\", \"up\", \"down\"], p=[0.8, 0.1, 0.1])\n",
    "        \n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position on board\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (self.state[0]-1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (self.state[0]+1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (self.state[0], self.state[1]-1)\n",
    "            else:\n",
    "                nxtState = (self.state[0], self.state[1]+1)\n",
    "            self.determine = False\n",
    "        else:\n",
    "            # non-deterministic\n",
    "            action = self._chooseActionProb(action)\n",
    "            self.determine = True\n",
    "            nxtState = self.nxtPosition(action)\n",
    "                        \n",
    "        # if next state is legal\n",
    "        if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
    "            if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
    "                if nxtState != (1, 1):\n",
    "                    return nxtState\n",
    "        return self.state\n",
    "    \n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase encapsula la lógica necesaria para que un agente interactúe con un mundo cuadriculado en una configuración sencilla de aprendizaje por refuerzo. La elección probabilística de acciones introduce un elemento de incertidumbre que hace que el entorno sea más complejo y realista. La implementación de reglas de movimiento, la estructura de recompensas y la comprobación del estado terminal son componentes esenciales para simular y resolver problemas de aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define una clase Agente diseñada para interactuar con la clase State en un entorno de mundo cuadriculado, como se ha comentado anteriormente. El agente aprende a navegar por el entorno utilizando Q-learning, un algoritmo de aprendizaje por refuerzo sin modelos. La clase está estructurada para permitir que el agente explore el entorno, aprenda de sus experiencias y actualice sus conocimientos sobre las acciones más gratificantes a realizar en diferentes estados. Desglosemos los componentes clave de la clase Agente:\n",
    "\n",
    "**__init__(self):**\n",
    "\n",
    "* Inicializa el agente con una lista vacía de estados para registrar las posiciones y acciones realizadas.\n",
    "* Define las posibles acciones que puede realizar el agente: arriba, abajo, izquierda y derecha.\n",
    "* Inicializa el estado actual del agente creando una instancia de la clase State.\n",
    "* Establece los parámetros de aprendizaje: tasa de aprendizaje (lr), tasa de exploración (exp_rate) y factor de descuento (decay_gamma).\n",
    "* Inicializa a cero los valores Q de cada par estado-acción. Los valores Q se almacenan en un diccionario de diccionarios, donde las claves externas del diccionario son tuplas de estado y las claves internas del diccionario son acciones.\n",
    "\n",
    "**chooseAction(self):**\n",
    "\n",
    "* Decide la siguiente acción a tomar basándose en la política actual. Con una probabilidad igual a la tasa de exploración (exp_rate), el agente elige aleatoriamente una acción para explorar el entorno. En caso contrario, elige la acción con la mayor recompensa esperada (acción codiciosa) basándose en sus valores Q actuales.\n",
    "\n",
    "**takeAction(self, action):**\n",
    "\n",
    "* Actualiza la posición del agente en función de la acción realizada. Utiliza el método nxtPosition de la clase State para calcular el siguiente estado y devuelve una nueva instancia de State que representa este siguiente estado.\n",
    "\n",
    "**reset(self):**\n",
    "\n",
    "* Restablece el agente a su estado inicial borrando la lista de estados registrados y reinicializando la instancia State. Este método se utiliza al final de cada episodio para iniciar una nueva partida.\n",
    "\n",
    "**play(self, rounds=10):**\n",
    "* El bucle principal donde el agente juega el juego durante un número especificado de rondas. En cada ronda, el agente elige repetidamente acciones, actualiza su estado y registra los pares estado-acción hasta que el juego termina (es decir, alcanza un estado terminal).\n",
    "* Cuando el juego termina, el agente actualiza los valores Q de todos los pares estado-acción encontrados en el episodio en orden inverso (retropropagación). La recompensa recibida al final del juego se utiliza para actualizar los valores Q, ajustados por la tasa de aprendizaje (lr) y el factor de descuento (decay_gamma). Este proceso se conoce como aprendizaje por diferencia temporal (TD).\n",
    "* Tras actualizar los valores Q, el agente restablece su estado para iniciar un nuevo episodio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []  # record position and action taken at the position\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        self.decay_gamma = 0.9\n",
    "\n",
    "        # initial Q values\n",
    "        self.Q_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.Q_values[(i, j)] = {}\n",
    "                for a in self.actions:\n",
    "                    self.Q_values[(i, j)][a] = 0  # Q value is a dict of dict  \n",
    "    \n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "        \n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                current_position = self.State.state\n",
    "                nxt_reward = self.Q_values[current_position][a]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n",
    "        return action\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        # update State\n",
    "        return State(state=position)     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "    \n",
    "    def play(self, rounds=10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of game back propagate reward\n",
    "            if self.State.isEnd:\n",
    "                # back propagate\n",
    "                reward = self.State.giveReward()\n",
    "                for a in self.actions:\n",
    "                    self.Q_values[self.State.state][a] = reward\n",
    "                print(\"Game End Reward\", reward)\n",
    "                for s in reversed(self.states):\n",
    "                    current_q_value = self.Q_values[s[0]][s[1]]\n",
    "                    reward = current_q_value + self.lr*(self.decay_gamma*reward - current_q_value)\n",
    "                    self.Q_values[s[0]][s[1]] = round(reward, 3)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.chooseAction()\n",
    "                # append trace\n",
    "                self.states.append([(self.State.state), action])\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.takeAction(action)\n",
    "                # mark is end\n",
    "                self.State.isEndFunc()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                print(\"---------------------\")\n",
    "                self.isEnd = self.State.isEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptos clave:**\n",
    "\n",
    "* Exploración vs. Explotación: El agente utiliza una política de epsilon-greedy para la selección de acciones, equilibrando entre la exploración de nuevas acciones con una elección aleatoria y la explotación de acciones conocidas con los valores Q más altos.\n",
    "* Actualización del aprendizaje Q: Los valores Q se actualizan mediante la fórmula Q(s,a)=Q(s,a)+α[r+γmaxa′Q(s′,a′)-Q(s,a)]Q(s,a)=Q(s,a)+α[r+γmaxa′Q(s′,a′)-Q(s,a)], donde αα es la tasa de aprendizaje, rr es la recompensa, γγ es el factor de descuento, s′s′ es el siguiente estado, y a′a′ son las posibles acciones en el siguiente estado. Esta fórmula se simplifica en el código para actualizar los valores Q en función de la recompensa recibida al final del juego, lo que demuestra una forma básica de la regla de actualización del aprendizaje Q.\n",
    "\n",
    "Esta clase encapsula el comportamiento de aprendizaje de un agente en un entorno de mundo cuadriculado, utilizando el aprendizaje Q para mejorar su política de navegación a través de la cuadrícula para alcanzar su objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (0, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (0, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (0, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (2, 0): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (2, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (2, 2): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (2, 3): {'up': 0, 'down': 0, 'left': 0, 'right': 0}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action right\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action right\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action right\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action right\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action right\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action down\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action up\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action left\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action down\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n"
     ]
    }
   ],
   "source": [
    "ag.play(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.276, 'down': 0.144, 'left': 0.251, 'right': 0.414},\n",
       " (0, 1): {'up': 0.296, 'down': 0.354, 'left': 0.212, 'right': 0.71},\n",
       " (0, 2): {'up': 0.45, 'down': 0.019, 'left': 0.26, 'right': 0.881},\n",
       " (0, 3): {'up': 1, 'down': 1, 'left': 1, 'right': 1},\n",
       " (1, 0): {'up': 0.351, 'down': 0.107, 'left': 0.19, 'right': 0.029},\n",
       " (1, 1): {'up': 0, 'down': 0, 'left': 0, 'right': 0},\n",
       " (1, 2): {'up': 0.34, 'down': -0.005, 'left': 0.122, 'right': -0.144},\n",
       " (1, 3): {'up': -1, 'down': -1, 'left': -1, 'right': -1},\n",
       " (2, 0): {'up': 0.269, 'down': 0.136, 'left': 0.119, 'right': 0.047},\n",
       " (2, 1): {'up': -0.001, 'down': 0.01, 'left': 0.158, 'right': -0.004},\n",
       " (2, 2): {'up': 0, 'down': 0, 'left': 0.008, 'right': -0.026},\n",
       " (2, 3): {'up': 0.0, 'down': 0, 'left': 0.001, 'right': -0.176}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag.Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de hacerlo probabilista sin que las acciones sean aleatorias es de la siguiente manera: que los estados estén determinados por 4 sensores que tiene el agente, cada uno le dice si hay una pared a su alrededor o no. Así no siempre sabe dónde está y podría estar en un estado, hacer una acción y no siempre ir al mismo sitio. **Intentar resolver el problema por este método.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) ¿Qué es Q-learning y en qué tipo de problemas se aplica?\n",
    "2) Identifica y describe los componentes principales de un algoritmo Q-learning.\n",
    "3) ¿Cómo se actualizan los valores Q en el algoritmo Q-learning? Proporciona la fórmula y explica cada uno de sus componentes.\n",
    "4) Describe la política epsilon-greedy y explica cómo equilibra la exploración y la explotación.\n",
    "5) ¿Por qué es importante la función de recompensa en Q-learning? Describe cómo influye en el aprendizaje del agente.\n",
    "6) Dado un conjunto de valores Q para un estado específico y diferentes acciones, ¿cómo decidiría un agente la próxima acción a realizar?\n",
    "7) ¿Bajo qué condiciones se espera que Q-learning converja a la política óptima?\n",
    "8) ¿Cómo afecta el valor del factor de descuento (γγ) en el comportamiento de aprendizaje de un agente Q-learning?\n",
    "9) ¿Qué efecto tiene la tasa de aprendizaje (αα) en el proceso de actualización de Q-learning y cómo debería ser seleccionada?\n",
    "10) ¿Cómo se puede adaptar Q-learning para manejar entornos con espacios de estados y/o acciones continuos?\n",
    "11) Explica por qué Q-learning es considerado un algoritmo off-policy y qué implica esto en términos de aprendizaje.\n",
    "12) Discute cómo la inicialización de los valores Q puede afectar el aprendizaje y la exploración del agente.\n",
    "13) Proporciona ejemplos de cómo se ha aplicado Q-learning en problemas del mundo real. ¿Cuáles son algunos de los desafíos encontrados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptado de: \n",
    "\n",
    "* https://github.com/MJeremy2017/reinforcement-learning-implementation/tree/master/GridWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A Beginners Guide to Q-Learning: https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c\n",
    "\n",
    "* Knowledge transfer in Reinforcement Learning ...can RL be a little easier?: https://regressionist.github.io/2019-05-13-Reinforcement-Learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artículos de interés"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DeepMind AI Reduces Google Data Centre Cooling Bill by 40%: https://deepmind.google/discover/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-by-40/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lenguaje Matemático](./Images/Matematicas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contacto](./Images/Contacto.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
