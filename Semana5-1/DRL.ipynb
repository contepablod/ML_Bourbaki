{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning & AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este proyecto aprovecha un modelo de aprendizaje profundo de IA para optimizar y reducir el consumo energético de un centro de datos hasta en un 70%. El modelo de IA utiliza el algoritmo Q-Learning para determinar la mejor acción en cada paso temporal. Q-Learning se basa en las ecuaciones de Bellman, que son la base del aprendizaje por refuerzo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proyecto pretende reducir el consumo energético de una instalación industrial. Se utiliza un modelo de optimización de aprendizaje profundo y se compara con el sistema de refrigeración integrado tradicional. El enfoque se inspira en la reducción del 40% lograda en los centros de datos de Google utilizando el modelo de IA de DeepMind. El proyecto forma parte del módulo Artificial Intelligence for Business de Udemy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**En este escenario, hay dos supuestos clave:**\n",
    "\n",
    "* La temperatura intrínseca de un servidor es una función de la temperatura atmosférica, el número de usuarios en el servidor y la velocidad de transmisión de datos. La relación se aproxima mediante una combinación lineal de esas 3 variables. Los coeficientes se estiman mediante un análisis de regresión.\n",
    "\n",
    "* La energía gastada para regular la temperatura entre dos pasos temporales es proporcional al cambio absoluto de temperatura. Utilizando esta relación lineal, podemos estimar que el consumo de energía de cada mecanismo es proporcional al cambio absoluto de temperatura del servidor. Esto se aplica tanto al sistema de IA como al sistema de refrigeración integrado tradicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning es un algoritmo de aprendizaje por refuerzo para aprender la calidad de las acciones que indica a un agente qué acción debe realizar en qué circunstancias. Determina el valor de todas las acciones posibles dado un determinado estado (o circunstancias). No requiere un modelo del entorno, ya que lo aprende mientras lo explora. Los valores de las acciones (o Calidad de las acciones) se determinan recursivamente a medida que el algoritmo explora el entorno y aprende de los resultados obtenidos a lo largo de un elevado número de iteraciones. El aprendizaje Q encuentra una política óptima (secuencia de acciones) que maximiza el valor esperado de la recompensa total en todos y cada uno de los pasos sucesivos, partiendo del estado actual. En otras palabras, el aprendizaje Q puede identificar una política óptima de selección de acciones para cualquier proceso de decisión de Markov finito (no depende del pasado, sino únicamente de las acciones futuras), dado un tiempo de exploración infinito y una política parcialmente aleatoria.\n",
    "\n",
    "Así, la ecuación de Bellman descompone el valor en dos partes, la recompensa inmediata más los valores futuros descontados. La ecuación de Bellman aparece por todas partes en la literatura del Aprendizaje por Refuerzo, siendo uno de los elementos centrales de muchos algoritmos de Aprendizaje por Refuerzo.\n",
    "\n",
    "En este proyecto, la recompensa se define como la diferencia absoluta entre la energía requerida por el sistema de refrigeración y la energía requerida por el modelo de IA. Esta es la energía ahorrada por la IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proyecto utiliza una red neuronal sencilla formada por 3 capas totalmente conectadas. La red toma como entrada un vector normalizado que representa el estado. En este problema, el estado está representado por la temperatura del servidor, el número de usuarios y la velocidad de transmisión de datos. El estado se actualiza en cada paso temporal.\n",
    "\n",
    "Las dos capas ocultas tienen 64 y 32 nodos respectivamente.\n",
    "\n",
    "La capa de salida predice los valores Q para 5 acciones potenciales que cubren las opciones disponibles para el sistema. Una función de activación softmax genera una distribución de probabilidad sobre las acciones. La probabilidad más alta corresponde al valor Q más alto.\n",
    "\n",
    "La fase de aprendizaje utiliza la técnica \"Experience Replay\" para entrenar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Utils\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define un entorno simulado para un escenario de aprendizaje por refuerzo (RL) en el que el objetivo es gestionar la temperatura de una sala de servidores de forma eficiente mediante el uso de inteligencia artificial (IA). Simula la dinámica del entorno, incluidos los cambios de temperatura debidos a las condiciones atmosféricas, el número de usuarios y la velocidad de procesamiento de datos, que en conjunto influyen en la temperatura de la sala de servidores. El agente de IA interactúa con este entorno realizando acciones para ajustar el sistema de refrigeración del servidor, con el objetivo de mantener la temperatura dentro de un rango óptimo. La eficacia de estas acciones se evalúa mediante un sistema de recompensas que penaliza el uso excesivo de energía y recompensa la gestión eficiente de la temperatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase Environment encapsula todos los parámetros, variables y métodos necesarios para simular el escenario de gestión de la temperatura de la sala de servidores. Desglosemos los métodos:\n",
    "\n",
    "**__init__:**\n",
    "\n",
    "Inicializa el entorno con los siguientes parámetros:\n",
    "\n",
    "* optimal_temperature: El rango de temperatura objetivo para la sala de servidores.\n",
    "* initial_month: El mes inicial, que afecta a la temperatura ambiente.\n",
    "* initial_number_users: El número inicial de usuarios que utilizan los recursos del servidor.\n",
    "* initial_rate_data: La tasa inicial de datos que procesa el servidor.\n",
    "\n",
    "Estos parámetros influyen en el estado inicial del entorno, incluyendo la temperatura atmosférica, la temperatura intrínseca de la sala del servidor (basada en los usuarios actuales y la tasa de datos), y la temperatura inicial establecida por los sistemas AI y no AI.\n",
    "\n",
    "**update_env:**\n",
    "\n",
    "Simula el efecto de una acción realizada por la IA para ajustar la temperatura. Actualiza el estado del entorno en función de la dirección y la magnitud de la acción, el cambio en el número de usuarios y la velocidad de procesamiento de datos. También calcula la nueva recompensa basándose en la energía utilizada por la IA y el sistema no IA, actualiza la temperatura de la sala del servidor y comprueba si el juego ha terminado debido a que la temperatura ha caído fuera del rango permitido.\n",
    "\n",
    "**reset:**\n",
    "\n",
    "Reinicia el entorno a un mes especificado, reinicializando la temperatura atmosférica y otras variables de estado a sus valores iniciales. Este método se utiliza para iniciar un nuevo episodio en el contexto RL.\n",
    "\n",
    "**observe:**\n",
    "\n",
    "Proporciona el estado actual del entorno al agente de la IA. Esto incluye las versiones escaladas de la temperatura del servidor, el número de usuarios y la tasa de procesamiento de datos, junto con la última recompensa recibida y si el proceso ha terminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    # introduce and initialize all paramaters and variables of the environment\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimal_temperature=[18.0, 24.0],\n",
    "        initial_month=0,\n",
    "        initial_number_users=10,\n",
    "        initial_rate_data=60,\n",
    "    ):\n",
    "        self.initial_month = initial_month\n",
    "\n",
    "        self.monthly_atmospheric_temperatures = [\n",
    "            1.0,\n",
    "            5.0,\n",
    "            7.0,\n",
    "            10.0,\n",
    "            11.0,\n",
    "            20.0,\n",
    "            23.0,\n",
    "            24.0,\n",
    "            22.0,\n",
    "            10.0,\n",
    "            5.0,\n",
    "            1.0,\n",
    "        ]\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[\n",
    "            initial_month\n",
    "        ]\n",
    "        self.optimal_temperature = optimal_temperature\n",
    "        self.min_temperature = -20\n",
    "        self.max_temperature = 80\n",
    "\n",
    "        self.min_number_users = 10\n",
    "        self.max_number_users = 100\n",
    "        self.max_update_users = 5\n",
    "        self.initial_number_users = initial_number_users\n",
    "        self.current_number_users = initial_number_users\n",
    "\n",
    "        self.min_rate_data = 20\n",
    "        self.max_rate_data = 300\n",
    "        self.max_update_data = 10\n",
    "        self.initial_rate_data = initial_rate_data\n",
    "        self.current_rate_data = initial_rate_data\n",
    "\n",
    "        self.intrinsic_temperature = (\n",
    "            self.atmospheric_temperature\n",
    "            + 1.25 * self.current_number_users\n",
    "            + 1.25 * self.current_rate_data\n",
    "        )\n",
    "        self.temperature_ai = self.intrinsic_temperature\n",
    "        self.temperature_noai = (\n",
    "            self.optimal_temperature[0] + self.optimal_temperature[1]\n",
    "        ) / 2.0  # mid of optimal range\n",
    "\n",
    "        self.total_energy_ai = 0.0\n",
    "        self.total_energy_noai = 0.0\n",
    "\n",
    "        self.reward = 0.0\n",
    "        self.game_over = 0\n",
    "        self.train = 1  # train or inference mode\n",
    "\n",
    "    # method to update environment after AI plays an action\n",
    "    def update_env(self, direction, energy_ai, month):\n",
    "        \"\"\"variables:\n",
    "        - direction :  change of temperature by AI incr or decr +1 or -1\"\"\"\n",
    "\n",
    "        # GETTING THE REWARD\n",
    "        # Computing the energy spent by the server's cooling system when there is no AI\n",
    "        energy_noai = 0\n",
    "        if self.temperature_noai < self.optimal_temperature[0]:\n",
    "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
    "            self.temperature_noai = self.optimal_temperature[0]\n",
    "        elif self.temperature_noai > self.optimal_temperature[1]:\n",
    "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
    "            self.temperature_noai = self.optimal_temperature[1]\n",
    "        # Computing the Reward and Scaling the Reward\n",
    "        self.reward = energy_noai - energy_ai\n",
    "        self.reward = 1e-3 * self.reward\n",
    "\n",
    "        # GETTING NEXT STATE\n",
    "        # Updating the atmospheric temperature\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
    "        # Updating the number of users between the min / max range\n",
    "        self.current_number_users += np.random.randint(\n",
    "            -self.max_update_users, self.max_update_users\n",
    "        )\n",
    "        if self.current_number_users > self.max_number_users:\n",
    "            self.current_number_users = self.max_number_users\n",
    "        elif self.current_number_users < self.min_number_users:\n",
    "            self.current_number_users = self.min_number_users\n",
    "        # Updating the rate of data between the min / max range\n",
    "        self.current_rate_data += np.random.randint(\n",
    "            -self.max_update_data, self.max_update_data\n",
    "        )\n",
    "        if self.current_rate_data > self.max_rate_data:\n",
    "            self.current_rate_data = self.max_rate_data\n",
    "        elif self.current_rate_data < self.min_rate_data:\n",
    "            self.current_rate_data = self.min_rate_data\n",
    "        # Computing the Delta of Intrinsic Temperature\n",
    "        past_intrinsic_temperature = (\n",
    "            self.intrinsic_temperature\n",
    "        )  # T° of server before action\n",
    "        self.intrinsic_temperature = (\n",
    "            self.atmospheric_temperature\n",
    "            + 1.25 * self.current_number_users\n",
    "            + 1.25 * self.current_rate_data\n",
    "        )  # T° of server updated\n",
    "        delta_intrinsic_temperature = (\n",
    "            self.intrinsic_temperature - past_intrinsic_temperature\n",
    "        )\n",
    "        # Computing the Delta of Temperature caused by the AI action\n",
    "        if direction == -1:\n",
    "            delta_temperature_ai = (\n",
    "                -energy_ai\n",
    "            )  # energy cost = abs delta of T° change by assumption\n",
    "        elif direction == 1:\n",
    "            delta_temperature_ai = energy_ai\n",
    "        # Updating the new Server's Temperature when there is the AI\n",
    "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
    "        # Updating the new Server's Temperature when there is no AI\n",
    "        self.temperature_noai += delta_intrinsic_temperature\n",
    "\n",
    "        # GETTING GAME OVER (allows to end of an epoch if T° out of bound during training)\n",
    "        if self.temperature_ai < self.min_temperature:\n",
    "            if self.train == 1:\n",
    "                self.game_over = 1\n",
    "            else:\n",
    "                self.total_energy_ai += (\n",
    "                    self.optimal_temperature[0] - self.temperature_ai\n",
    "                )\n",
    "                self.temperature_ai = self.optimal_temperature[0]\n",
    "        elif self.temperature_ai > self.max_temperature:\n",
    "            if self.train == 1:\n",
    "                self.game_over = 1\n",
    "            else:\n",
    "                self.total_energy_ai += (\n",
    "                    self.temperature_ai - self.optimal_temperature[1]\n",
    "                )\n",
    "                self.temperature_ai = self.optimal_temperature[1]\n",
    "\n",
    "        # UPDATING THE SCORES\n",
    "        self.total_energy_ai += energy_ai\n",
    "        self.total_energy_noai += energy_noai\n",
    "\n",
    "        # NORMALIZE NEXT STATE (state vector to be fed to neural network)\n",
    "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (\n",
    "            self.max_temperature - self.min_temperature\n",
    "        )\n",
    "        scaled_number_users = (self.current_number_users - self.min_number_users) / (\n",
    "            self.max_number_users - self.min_number_users\n",
    "        )\n",
    "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (\n",
    "            self.max_rate_data - self.min_rate_data\n",
    "        )\n",
    "        # create vector for updated state\n",
    "        next_state = np.matrix(\n",
    "            [scaled_temperature_ai, scaled_number_users, scaled_rate_data]\n",
    "        )\n",
    "\n",
    "        return next_state, self.reward, self.game_over\n",
    "\n",
    "    # METHOD THAT RESETS THE ENVIRONMENT\n",
    "    def reset(self, new_month):\n",
    "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
    "        self.initial_month = new_month\n",
    "        self.current_number_users = self.initial_number_users\n",
    "        self.current_rate_data = self.initial_rate_data\n",
    "        self.intrinsic_temperature = (\n",
    "            self.atmospheric_temperature\n",
    "            + 1.25 * self.current_number_users\n",
    "            + 1.25 * self.current_rate_data\n",
    "        )\n",
    "        self.temperature_ai = self.intrinsic_temperature\n",
    "        self.temperature_noai = (\n",
    "            self.optimal_temperature[0] + self.optimal_temperature[1]\n",
    "        ) / 2.0\n",
    "        self.total_energy_ai = 0.0\n",
    "        self.total_energy_noai = 0.0\n",
    "        self.reward = 0.0\n",
    "        self.game_over = 0\n",
    "        self.train = 1\n",
    "\n",
    "    # METHOD PROVIDING CURRENT STATE, LAST REWARD AND WHETHER THE GAME IS OVER\n",
    "    def observe(self):\n",
    "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (\n",
    "            self.max_temperature - self.min_temperature\n",
    "        )\n",
    "        scaled_number_users = (self.current_number_users - self.min_number_users) / (\n",
    "            self.max_number_users - self.min_number_users\n",
    "        )\n",
    "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (\n",
    "            self.max_rate_data - self.min_rate_data\n",
    "        )\n",
    "        # calc vector of current state\n",
    "        current_state = np.matrix(\n",
    "            [scaled_temperature_ai, scaled_number_users, scaled_rate_data]\n",
    "        )\n",
    "\n",
    "        return current_state, self.reward, self.game_over"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dinámica de simulación:**\n",
    "\n",
    "El entorno simula la dinámica de una sala de servidores en la que la temperatura puede variar en función de las condiciones externas (temperatura atmosférica), la carga interna (número de usuarios y velocidad de datos) y las intervenciones de la IA (ajustes en el sistema de refrigeración). El objetivo de la IA es gestionar estas dinámicas para mantener la temperatura dentro de un rango óptimo, minimizando el consumo de energía y evitando al mismo tiempo el sobrecalentamiento o la refrigeración excesiva.\n",
    "\n",
    "**Contexto de aprendizaje por refuerzo:**\n",
    "\n",
    "Este entorno simulado está diseñado para entrenar a un agente de IA mediante aprendizaje por refuerzo. El agente aprende a realizar acciones (ajustar la temperatura de la sala de servidores) basándose en el estado actual del entorno para maximizar la recompensa acumulada a lo largo del tiempo. El mecanismo de recompensa anima al agente a mantener la temperatura dentro del rango óptimo con un uso mínimo de energía, equilibrando el compromiso entre la eficiencia energética y la gestión eficaz de la temperatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Red Neuronal totalmente conectada con 2 capas ocultas (64 y luego 32 nodos)\n",
    "* Entrada : vector de estado (servidor T°, número de usuarios, tasa de datos)\n",
    "* Salida : Valores Q de las acciones de IA para regular la T° (reducir en 3°C o 1,5°, mantener la T°, Incr. en 1,5° o 3°C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(nn.Module):\n",
    "    def __init__(self, number_actions):\n",
    "        super(Brain, self).__init__()\n",
    "        self.number_actions = number_actions\n",
    "\n",
    "        # Define the neural network layers\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=3, out_features=64\n",
    "        )  # Input layer to first hidden layer\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=64, out_features=32\n",
    "        )  # First hidden layer to second hidden layer\n",
    "        self.fc3 = nn.Linear(\n",
    "            in_features=32, out_features=number_actions\n",
    "        )  # Second hidden layer to output layer\n",
    "\n",
    "    def forward(self, states):\n",
    "        x = torch.sigmoid(\n",
    "            self.fc1(states)\n",
    "        )  # Activation function for first hidden layer\n",
    "        x = torch.sigmoid(self.fc2(x))  # Activation function for second hidden layer\n",
    "        q_values = torch.softmax(\n",
    "            self.fc3(x), dim=-1\n",
    "        )  # Softmax activation for output layer\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código define una clase DQN (Deep Q-Network), que es un componente clave en el aprendizaje por refuerzo (RL) para entrenar a los agentes a tomar decisiones basadas en su estado actual para maximizar alguna noción de recompensa acumulativa. En concreto, esta clase se encarga de implementar el mecanismo de repetición de experiencias y de generar lotes de entrenamiento a partir de estas experiencias. \n",
    "\n",
    "La clase DQN inicializa y gestiona la memoria para la repetición de experiencias, una técnica utilizada para almacenar las experiencias del agente en cada paso de tiempo y posteriormente reproducir estas experiencias de forma aleatoria para romper la correlación entre muestras de aprendizaje consecutivas. Este enfoque estabiliza el proceso de aprendizaje. Desglosemos sus métodos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__init__(self, max_memory = 100, discount = 0.9):**\n",
    "\n",
    "Inicializa el objeto DQN con un tamaño de memoria y un factor de descuento especificados.\n",
    "* max_memory: El número máximo de experiencias pasadas (transiciones) que el DQN puede recordar. Una vez alcanzado este límite, las experiencias antiguas se olvidan para dejar espacio a las nuevas (FIFO - First In, First Out).\n",
    "* discount: Factor de descuento (a menudo denominado gamma en la literatura) utilizado para calcular la recompensa futura descontada. Representa la diferencia de importancia entre las recompensas futuras y las inmediatas.\n",
    "\n",
    "**remember(self, transition, game_over):**\n",
    "\n",
    "Almacena experiencias en la memoria. Cada experiencia se almacena como una tupla que contiene el estado actual, la acción realizada, la recompensa recibida y el siguiente estado. También almacena si el juego ha terminado después de esta transición.\n",
    "* transition: Una tupla de (estado actual, acción, recompensa, siguiente estado).\n",
    "* game_over: Una bandera binaria (0 o 1) que indica si el juego terminó después de la transición.\n",
    "\n",
    "Si la memoria excede max_memory, se elimina la memoria más antigua.\n",
    "\n",
    "**get_batch(self, model, batch_size = 10):**\n",
    "\n",
    "Construye lotes de entradas y objetivos para entrenar el modelo muestreando experiencias de la memoria.\n",
    "* model: El modelo de red neuronal utilizado para predecir los valores Q.\n",
    "* batch_size: El número de muestras a incluir en el lote.\n",
    "\n",
    "Inicializa dos matrices, inputs y targets, para almacenar los estados (inputs para el modelo) y los valores Q objetivo (objetivos para el entrenamiento) respectivamente.\n",
    "\n",
    "Selecciona aleatoriamente experiencias de la memoria y rellena las matrices de entradas y objetivos con el estado actual y los valores Q previstos.\n",
    "\n",
    "Para cada experiencia seleccionada, actualiza el valor Q de la acción realizada (objetivos[i, acción]) con la recompensa recibida más la recompensa futura máxima descontada (valor futuro esperado). Esta actualización sólo se realiza si el proceso no ha terminado; en caso contrario, el objetivo es simplemente la recompensa recibida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    # INITIALIZE ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
    "    def __init__(self, max_memory=100, discount=0.9):\n",
    "        self.memory = list()\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount  # discount factor used in calculating the targets Q\n",
    "\n",
    "    # METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
    "    def remember(self, transition, game_over):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        transition: tuple of 4 elements (current state, action played, reward received, next state)\n",
    "        game_over: 0 or 1\n",
    "        \"\"\"\n",
    "        self.memory.append((transition, game_over))\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            self.memory.pop(0)  # delete the first memory element (FIFO)\n",
    "\n",
    "    # CONSTRUCT BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
    "    def get_batch(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        sample_size = min(len_memory, batch_size)\n",
    "        # Randomly sample some transitions from memory for the batch\n",
    "        sample_indices = np.random.choice(len_memory, sample_size, replace=False)\n",
    "\n",
    "        # Initialize tensors for inputs and targets\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            current_state, action, reward, next_state = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            # Convert to PyTorch tensors and send to the specified device\n",
    "            current_state_tensor = torch.tensor(current_state, dtype=torch.float).to(\n",
    "                device\n",
    "            )\n",
    "            next_state_tensor = torch.tensor(next_state, dtype=torch.float).to(device)\n",
    "\n",
    "            with torch.no_grad():  # No gradient needed for the prediction step\n",
    "                target = (\n",
    "                    model(current_state_tensor.unsqueeze(0)).squeeze(1).cpu().numpy()\n",
    "                )\n",
    "                Q_next_state = (\n",
    "                    model(next_state_tensor.unsqueeze(0))\n",
    "                    .squeeze(1)\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                    .max(1)[0]\n",
    "                )\n",
    "                if game_over:\n",
    "                    target[0][action] = reward\n",
    "                else:\n",
    "                    target[0][action] = reward + self.discount * Q_next_state\n",
    "\n",
    "            inputs.append(current_state)\n",
    "            targets.append(target[0])\n",
    "\n",
    "        inputs = torch.tensor(np.array(inputs), dtype=torch.float).to(device)\n",
    "        targets = torch.tensor(np.array(targets), dtype=torch.float).to(device)\n",
    "\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING UP THE PARAMETERS\n",
    "epsilon = (\n",
    "    0.3  # exploration vs exploitation ratio. Here 30% exploration (random selection)\n",
    ")\n",
    "number_actions = 5\n",
    "direction_boundary = (\n",
    "    number_actions - 1\n",
    ") / 2  # boundary separating direction of T° change actions\n",
    "number_epochs = 100\n",
    "max_memory = 3000\n",
    "batch_size = 512\n",
    "temperature_step = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE ENVIRONMENT BY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = Environment(\n",
    "    optimal_temperature=(18.0, 24.0),\n",
    "    initial_month=0,\n",
    "    initial_number_users=20,\n",
    "    initial_rate_data=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE NEURAL NETWORK OBJECT USING BRAIN CLASS\n",
    "brain = Brain(number_actions=number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILDING THE DQN MODEL\n",
    "dqn = DQN(max_memory=max_memory, discount=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSING THE MODE\n",
    "train = True\n",
    "# TRAINING THE NN\n",
    "env.train = train\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain.to(device)\n",
    "optimizer = optim.Adam(brain.parameters(), lr=LR, amsgrad=True, weight_decay=LR*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/100 - Total Reward: 0.00225, Total Loss: 0.0097\n",
      "Total Energy spent with an AI: 3\n",
      "Total Energy spent with no AI: 5\n",
      "----------------------------------------------------------------------\n",
      "Epoch 002/100 - Total Reward: 0.03950, Total Loss: 0.0313\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 48\n",
      "----------------------------------------------------------------------\n",
      "Epoch 003/100 - Total Reward: 0.00725, Total Loss: 0.0095\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 16\n",
      "----------------------------------------------------------------------\n",
      "Epoch 004/100 - Total Reward: -0.00675, Total Loss: 0.0290\n",
      "Total Energy spent with an AI: 39\n",
      "Total Energy spent with no AI: 32\n",
      "----------------------------------------------------------------------\n",
      "Epoch 005/100 - Total Reward: 0.01925, Total Loss: 0.0074\n",
      "Total Energy spent with an AI: 15\n",
      "Total Energy spent with no AI: 34\n",
      "----------------------------------------------------------------------\n",
      "Epoch 006/100 - Total Reward: 0.05075, Total Loss: 0.0164\n",
      "Total Energy spent with an AI: 48\n",
      "Total Energy spent with no AI: 99\n",
      "----------------------------------------------------------------------\n",
      "Epoch 007/100 - Total Reward: 0.06500, Total Loss: 0.0188\n",
      "Total Energy spent with an AI: 64\n",
      "Total Energy spent with no AI: 130\n",
      "----------------------------------------------------------------------\n",
      "Epoch 008/100 - Total Reward: 0.06300, Total Loss: 0.0226\n",
      "Total Energy spent with an AI: 104\n",
      "Total Energy spent with no AI: 166\n",
      "----------------------------------------------------------------------\n",
      "Epoch 009/100 - Total Reward: 0.14050, Total Loss: 0.0333\n",
      "Total Energy spent with an AI: 138\n",
      "Total Energy spent with no AI: 278\n",
      "----------------------------------------------------------------------\n",
      "Epoch 010/100 - Total Reward: 0.19875, Total Loss: 0.0234\n",
      "Total Energy spent with an AI: 116\n",
      "Total Energy spent with no AI: 314\n",
      "----------------------------------------------------------------------\n",
      "Epoch 011/100 - Total Reward: 0.03300, Total Loss: 0.0082\n",
      "Total Energy spent with an AI: 66\n",
      "Total Energy spent with no AI: 99\n",
      "----------------------------------------------------------------------\n",
      "Epoch 012/100 - Total Reward: 0.02850, Total Loss: 0.0026\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 36\n",
      "----------------------------------------------------------------------\n",
      "Epoch 013/100 - Total Reward: 0.05000, Total Loss: 0.0215\n",
      "Total Energy spent with an AI: 130\n",
      "Total Energy spent with no AI: 180\n",
      "----------------------------------------------------------------------\n",
      "Epoch 014/100 - Total Reward: 0.16750, Total Loss: 0.0205\n",
      "Total Energy spent with an AI: 116\n",
      "Total Energy spent with no AI: 283\n",
      "----------------------------------------------------------------------\n",
      "Epoch 015/100 - Total Reward: 0.08000, Total Loss: 0.0092\n",
      "Total Energy spent with an AI: 42\n",
      "Total Energy spent with no AI: 122\n",
      "----------------------------------------------------------------------\n",
      "Epoch 016/100 - Total Reward: 0.06650, Total Loss: 0.0272\n",
      "Total Energy spent with an AI: 206\n",
      "Total Energy spent with no AI: 272\n",
      "----------------------------------------------------------------------\n",
      "Epoch 017/100 - Total Reward: 0.00875, Total Loss: 0.0011\n",
      "Total Energy spent with an AI: 0\n",
      "Total Energy spent with no AI: 9\n",
      "----------------------------------------------------------------------\n",
      "Epoch 018/100 - Total Reward: 0.25550, Total Loss: 0.0412\n",
      "Total Energy spent with an AI: 303\n",
      "Total Energy spent with no AI: 558\n",
      "----------------------------------------------------------------------\n",
      "Epoch 019/100 - Total Reward: 0.04675, Total Loss: 0.0080\n",
      "Total Energy spent with an AI: 57\n",
      "Total Energy spent with no AI: 104\n",
      "----------------------------------------------------------------------\n",
      "Epoch 020/100 - Total Reward: 0.03875, Total Loss: 0.0046\n",
      "Total Energy spent with an AI: 32\n",
      "Total Energy spent with no AI: 70\n",
      "----------------------------------------------------------------------\n",
      "Epoch 021/100 - Total Reward: 0.00625, Total Loss: 0.0011\n",
      "Total Energy spent with an AI: 10\n",
      "Total Energy spent with no AI: 17\n",
      "----------------------------------------------------------------------\n",
      "Epoch 022/100 - Total Reward: 0.01500, Total Loss: 0.0010\n",
      "Total Energy spent with an AI: 12\n",
      "Total Energy spent with no AI: 27\n",
      "----------------------------------------------------------------------\n",
      "Epoch 023/100 - Total Reward: 0.05975, Total Loss: 0.0141\n",
      "Total Energy spent with an AI: 110\n",
      "Total Energy spent with no AI: 169\n",
      "----------------------------------------------------------------------\n",
      "Epoch 024/100 - Total Reward: 0.01675, Total Loss: 0.0080\n",
      "Total Energy spent with an AI: 60\n",
      "Total Energy spent with no AI: 77\n",
      "----------------------------------------------------------------------\n",
      "Epoch 025/100 - Total Reward: 0.01400, Total Loss: 0.0019\n",
      "Total Energy spent with an AI: 10\n",
      "Total Energy spent with no AI: 24\n",
      "----------------------------------------------------------------------\n",
      "Epoch 026/100 - Total Reward: 0.00925, Total Loss: 0.0019\n",
      "Total Energy spent with an AI: 10\n",
      "Total Energy spent with no AI: 20\n",
      "----------------------------------------------------------------------\n",
      "Epoch 027/100 - Total Reward: 0.50300, Total Loss: 0.0873\n",
      "Total Energy spent with an AI: 628\n",
      "Total Energy spent with no AI: 1132\n",
      "----------------------------------------------------------------------\n",
      "Epoch 028/100 - Total Reward: 0.03850, Total Loss: 0.0144\n",
      "Total Energy spent with an AI: 122\n",
      "Total Energy spent with no AI: 160\n",
      "----------------------------------------------------------------------\n",
      "Epoch 029/100 - Total Reward: 0.01900, Total Loss: 0.0027\n",
      "Total Energy spent with an AI: 27\n",
      "Total Energy spent with no AI: 46\n",
      "----------------------------------------------------------------------\n",
      "Epoch 030/100 - Total Reward: 0.23025, Total Loss: 0.0280\n",
      "Total Energy spent with an AI: 207\n",
      "Total Energy spent with no AI: 437\n",
      "----------------------------------------------------------------------\n",
      "Epoch 031/100 - Total Reward: 0.06400, Total Loss: 0.0046\n",
      "Total Energy spent with an AI: 24\n",
      "Total Energy spent with no AI: 88\n",
      "----------------------------------------------------------------------\n",
      "Epoch 032/100 - Total Reward: 0.14800, Total Loss: 0.0168\n",
      "Total Energy spent with an AI: 144\n",
      "Total Energy spent with no AI: 292\n",
      "----------------------------------------------------------------------\n",
      "Epoch 033/100 - Total Reward: 0.07750, Total Loss: 0.0064\n",
      "Total Energy spent with an AI: 51\n",
      "Total Energy spent with no AI: 128\n",
      "----------------------------------------------------------------------\n",
      "Epoch 034/100 - Total Reward: 0.01575, Total Loss: 0.0015\n",
      "Total Energy spent with an AI: 10\n",
      "Total Energy spent with no AI: 26\n",
      "----------------------------------------------------------------------\n",
      "Epoch 035/100 - Total Reward: 0.00575, Total Loss: 0.0011\n",
      "Total Energy spent with an AI: 8\n",
      "Total Energy spent with no AI: 13\n",
      "----------------------------------------------------------------------\n",
      "Epoch 036/100 - Total Reward: 0.00200, Total Loss: 0.0005\n",
      "Total Energy spent with an AI: 3\n",
      "Total Energy spent with no AI: 5\n",
      "----------------------------------------------------------------------\n",
      "Epoch 037/100 - Total Reward: 0.07125, Total Loss: 0.0039\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 80\n",
      "----------------------------------------------------------------------\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "if env.train:\n",
    "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
    "    for epoch in range(number_epochs):\n",
    "        total_reward = 0\n",
    "        total_loss = 0.0\n",
    "        new_month = np.random.randint(0, 12)\n",
    "        env.reset(new_month=new_month)\n",
    "        game_over = False\n",
    "        current_state, _, _ = env.observe()\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float).to(device)\n",
    "        timestep = 0\n",
    "\n",
    "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
    "        while (not game_over) and timestep <= 5 * 30 * 24 * 60:\n",
    "            # PLAYING THE NEXT ACTION BY EXPLORATION OR INFERENCE\n",
    "            if np.random.rand() <= epsilon:  # Exploration\n",
    "                action = np.random.randint(0, number_actions)\n",
    "            else:  # Inference\n",
    "                with torch.no_grad():\n",
    "                    q_values = brain(current_state.unsqueeze(0))  # Add batch dimension\n",
    "                    action = q_values.squeeze().max(-1)[1].item()\n",
    "\n",
    "            # Determine direction and energy based on action\n",
    "            if action - direction_boundary < 0:\n",
    "                direction = -1\n",
    "            else:\n",
    "                direction = 1\n",
    "            energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "\n",
    "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "            next_state, reward, game_over = env.update_env(\n",
    "                direction, energy_ai, int(timestep / (30 * 24 * 60))\n",
    "            )\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float).to(device)\n",
    "            total_reward += reward\n",
    "\n",
    "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
    "            dqn.remember(\n",
    "                [current_state.cpu().numpy(), action, reward, next_state.cpu().numpy()],\n",
    "                game_over,\n",
    "            )\n",
    "\n",
    "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
    "            inputs, targets = dqn.get_batch(brain, batch_size=batch_size)\n",
    "\n",
    "            # FORWARD PASS\n",
    "            predictions = brain(inputs)\n",
    "\n",
    "            # COMPUTING THE LOSS\n",
    "            loss = F.mse_loss(predictions.squeeze(1), targets)\n",
    "\n",
    "            # BACKWARD AND OPTIMIZE\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            timestep += 1\n",
    "            current_state = next_state\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:03d}/{number_epochs:03d} - Total Reward: {total_reward:.5f}, Total Loss: {total_loss:.4f}\"\n",
    "        )\n",
    "        print(f\"Total Energy spent with an AI: {env.total_energy_ai:.0f}\")\n",
    "        print(f\"Total Energy spent with no AI: {env.total_energy_noai:.0f}\")\n",
    "        print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "        # EARLY STOPPING CRITERIA\n",
    "        if early_stopping:\n",
    "            if early_stopping:\n",
    "                if total_reward <= best_total_reward:\n",
    "                    patience_count += 1\n",
    "                elif total_reward > best_total_reward:\n",
    "                    best_total_reward = total_reward\n",
    "                    patience_count = 0\n",
    "                if patience_count >= patience:\n",
    "                    print(\"Early Stopping\")\n",
    "                    break\n",
    "\n",
    "        # SAVING MODEL\n",
    "        torch.save(brain.state_dict(), \"./Data/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de la performance del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating one year of energy management...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518400/518400 [03:36<00:00, 2390.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Energy spent with an AI: 1605478\n",
      "Total Energy spent with no AI: 1977996\n",
      "ENERGY SAVED WITH AI: 19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating one year of energy management...\")\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = Environment(\n",
    "    optimal_temperature=(18.0, 24.0),\n",
    "    initial_month=0,\n",
    "    initial_number_users=20,\n",
    "    initial_rate_data=30,\n",
    ")\n",
    "\n",
    "# LOAD PRE-TRAINED PYTORCH MODEL\n",
    "model = Brain(number_actions)  # Initialize your model architecture\n",
    "state_dict = torch.load(\"./Data/model.pth\")  # Load the state dictionary\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.to(device).eval()\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = False\n",
    "\n",
    "# RUNNING 1 YEAR SIMULATION IN INFERENCE MODE\n",
    "env.train = train\n",
    "current_state, _, _ = env.observe()\n",
    "current_state = (\n",
    "    torch.tensor(current_state, dtype=torch.float).unsqueeze(0).to(device)\n",
    ")  # Add batch dimension and move to device\n",
    "\n",
    "# STARTING THE LOOP OVER 1 YEAR\n",
    "for timestep in tqdm(range(12 * 30 * 24 * 60)):\n",
    "    with torch.no_grad():  # Inference only, no gradients needed\n",
    "        q_values = model(current_state)\n",
    "    action = torch.argmax(q_values[0]).item()  # Get the action with the highest Q-value\n",
    "\n",
    "    if action - direction_boundary < 0:\n",
    "        direction = -1\n",
    "    else:\n",
    "        direction = 1\n",
    "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
    "\n",
    "    # UPDATING ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "    next_state, _, _ = env.update_env(\n",
    "        direction, energy_ai, int(timestep / (30 * 24 * 60))\n",
    "    )  # month [0,11]\n",
    "   \n",
    "\n",
    "# PRINTING THE RESULTS FOR 1 YEAR\n",
    "print(f\"Total Energy spent with an AI: {env.total_energy_ai:.0f}\")\n",
    "print(f\"Total Energy spent with no AI: {env.total_energy_noai:.0f}\")\n",
    "print(\n",
    "    f\"ENERGY SAVED WITH AI: {(env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100:.0f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El porcentaje de energía ahorrada varía en función de los experimentos. El porcentaje se determina simulando un ciclo anual completo. Ambos modelos pretenden mantener el servidor dentro de un rango de temperatura óptimo de 18° a 24°C. La simulación se realiza para pasos de tiempo de un minuto a lo largo de un año completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Mostrar un gráfico donde podamos ver la evolución de la energía en el tiempo tanto para la energía gastada con y sin AI.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conceptuales y Teóricas:**\n",
    "* ¿Qué es el Deep Q-Learning y cómo se diferencia del Q-Learning tradicional?\n",
    "* Explica el concepto de \"replay memory\" en Deep Q-Learning. ¿Por qué es importante?\n",
    "* ¿Qué papel juega la función de pérdida (loss function) en el entrenamiento de una red Q en Deep Q-Learning?\n",
    "* Describe cómo la técnica de \"experience replay\" mejora el proceso de aprendizaje en el Deep Q-Learning.\n",
    "* ¿Qué es el \"discount factor\" en el contexto de Deep Q-Learning y cómo afecta al aprendizaje del agente?\n",
    "\n",
    "**Sobre el Código y la Implementación:**\n",
    "* En el código proporcionado para el problema de gestión energética, ¿cómo se determina la acción que debe tomar el agente en cada paso de tiempo?\n",
    "* Describe cómo se actualiza el entorno después de que el agente toma una acción. ¿Qué información se utiliza para calcular el nuevo estado y la recompensa?\n",
    "* Explica cómo se calcula la recompensa en este problema específico. ¿Qué incentivos y penalizaciones se consideran para el agente?\n",
    "* ¿Cómo se maneja la exploración frente a la explotación en este problema de Deep Q-Learning?\n",
    "\n",
    "**Aplicaciones y Análisis Crítico:**\n",
    "* ¿Cuáles son las ventajas y desventajas de usar Deep Q-Learning en la gestión de la energía de un centro de datos?\n",
    "* Considerando el problema de gestión energética, ¿qué modificaciones sugerirías para mejorar el rendimiento del modelo de Deep Q-Learning?\n",
    "* Discute cómo cambiarías la arquitectura de la red neuronal para este problema específico y por qué.\n",
    "* Analiza el impacto potencial de variar el tamaño del \"replay memory\" y el \"batch size\" en el rendimiento del aprendizaje.\n",
    "* ¿Cómo podrías modificar el problema para hacerlo más complejo o realista? ¿Qué cambios en el modelo o el algoritmo serían necesarios?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptado de: \n",
    "\n",
    "* https://github.com/LaurentVeyssier/Minimize-Energy-consumption-with-Deep-Learning-model/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lenguaje Matemático](./Images/Matematicas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Contacto](./Images/Contacto.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
